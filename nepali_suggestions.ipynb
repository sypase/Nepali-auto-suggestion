{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import Levenshtein\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_nepali_words():\n",
    "    \"\"\"Load Nepali words from the corpus file.\"\"\"\n",
    "    with open(\"data/nepali_corpus.txt\", encoding='utf-8') as f:\n",
    "        words = set(line.strip() for line in f if line.strip())\n",
    "    return words\n",
    "\n",
    "def create_character_mapping(words):\n",
    "    \"\"\"Create character to index mapping for the model.\"\"\"\n",
    "    chars = set()\n",
    "    for word in words:\n",
    "        chars.update(word)\n",
    "    char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
    "    char_to_idx['<PAD>'] = 0\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "def prepare_training_data(words, char_to_idx, max_len=20):\n",
    "    \"\"\"Prepare training data for the model.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Create sequences for each word\n",
    "        for i in range(1, len(word)):\n",
    "            input_seq = word[:i]\n",
    "            target_char = word[i]\n",
    "            \n",
    "            # Convert to indices\n",
    "            input_indices = [char_to_idx[char] for char in input_seq]\n",
    "            target_index = char_to_idx[target_char]\n",
    "            \n",
    "            X.append(input_indices)\n",
    "            y.append(target_index)\n",
    "    \n",
    "    # Pad sequences\n",
    "    X = pad_sequences(X, maxlen=max_len, padding='pre')\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=len(char_to_idx))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def build_model(vocab_size, max_len):\n",
    "    \"\"\"Build the LSTM model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 64, input_length=max_len),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        LSTM(64),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(words, epochs=50, batch_size=32):\n",
    "    \"\"\"Train the word suggestion model.\"\"\"\n",
    "    print(\"Preparing data for training...\")\n",
    "    char_to_idx, idx_to_char = create_character_mapping(words)\n",
    "    X, y = prepare_training_data(words, char_to_idx)\n",
    "    \n",
    "    print(\"Building and training model...\")\n",
    "    model = build_model(len(char_to_idx), X.shape[1])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X, y, \n",
    "                       epochs=epochs, \n",
    "                       batch_size=batch_size,\n",
    "                       verbose=1)\n",
    "    \n",
    "    # Save the model and mappings\n",
    "    model.save('nepali_word_model.h5')\n",
    "    with open('char_mappings.pkl', 'wb') as f:\n",
    "        pickle.dump((char_to_idx, idx_to_char), f)\n",
    "    \n",
    "    return model, char_to_idx, idx_to_char\n",
    "\n",
    "def load_trained_model():\n",
    "    \"\"\"Load the trained model and character mappings.\"\"\"\n",
    "    if not os.path.exists('nepali_word_model.h5') or not os.path.exists('char_mappings.pkl'):\n",
    "        return None, None, None\n",
    "    \n",
    "    model = tf.keras.models.load_model('nepali_word_model.h5')\n",
    "    with open('char_mappings.pkl', 'rb') as f:\n",
    "        char_to_idx, idx_to_char = pickle.load(f)\n",
    "    return model, char_to_idx, idx_to_char\n",
    "\n",
    "def get_ai_suggestions(partial_word, model, char_to_idx, idx_to_char, max_len=20, num_suggestions=5):\n",
    "    \"\"\"Get AI-based word suggestions.\"\"\"\n",
    "    if not partial_word:\n",
    "        return []\n",
    "    \n",
    "    # Convert input to indices\n",
    "    input_seq = [char_to_idx.get(char, 0) for char in partial_word]\n",
    "    input_seq = pad_sequences([input_seq], maxlen=max_len, padding='pre')\n",
    "    \n",
    "    # Generate suggestions\n",
    "    suggestions = set()\n",
    "    current_word = partial_word\n",
    "    \n",
    "    for _ in range(10):  # Try to generate 10 different suggestions\n",
    "        if len(suggestions) >= num_suggestions:\n",
    "            break\n",
    "            \n",
    "        # Predict next character\n",
    "        pred = model.predict(input_seq, verbose=0)\n",
    "        next_char_idx = np.argmax(pred[0])\n",
    "        next_char = idx_to_char[next_char_idx]\n",
    "        \n",
    "        # Add to current word\n",
    "        current_word += next_char\n",
    "        \n",
    "        # If we've generated a complete word, add it to suggestions\n",
    "        if current_word in words:\n",
    "            suggestions.add(current_word)\n",
    "        \n",
    "        # Update input sequence for next prediction\n",
    "        input_seq = [char_to_idx.get(char, 0) for char in current_word]\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_len, padding='pre')\n",
    "    \n",
    "    return list(suggestions)[:num_suggestions]\n",
    "\n",
    "def get_word_completions(partial_word, word_list, max_suggestions=5):\n",
    "    \"\"\"Get word completions for a partial Nepali word.\"\"\"\n",
    "    completions = [word for word in word_list if word.startswith(partial_word)]\n",
    "    return sorted(completions)[:max_suggestions]\n",
    "\n",
    "def get_spelling_suggestions(word, word_list, max_distance=2, max_suggestions=5):\n",
    "    \"\"\"Get spelling suggestions for a potentially misspelled word.\"\"\"\n",
    "    suggestions = []\n",
    "    for dict_word in word_list:\n",
    "        distance = Levenshtein.distance(word, dict_word)\n",
    "        if distance <= max_distance:\n",
    "            suggestions.append((dict_word, distance))\n",
    "    \n",
    "    # Sort by distance and return top suggestions\n",
    "    return [word for word, _ in sorted(suggestions, key=lambda x: x[1])[:max_suggestions]]\n",
    "\n",
    "def get_suggestions(input_text, words, model=None, char_to_idx=None, idx_to_char=None):\n",
    "    \"\"\"Get both completions and spelling suggestions for input text.\"\"\"\n",
    "    # Get traditional suggestions\n",
    "    completions = get_word_completions(input_text, words)\n",
    "    spelling_suggestions = get_spelling_suggestions(input_text, words)\n",
    "    \n",
    "    # Get AI-based suggestions if model is available\n",
    "    ai_suggestions = []\n",
    "    if model is not None and char_to_idx is not None and idx_to_char is not None:\n",
    "        ai_suggestions = get_ai_suggestions(input_text, model, char_to_idx, idx_to_char)\n",
    "    \n",
    "    # Combine all suggestions\n",
    "    all_suggestions = list(set(completions + spelling_suggestions + ai_suggestions))\n",
    "    return all_suggestions[:5]  # Return top 5 suggestions\n",
    "\n",
    "def interactive_demo():\n",
    "    \"\"\"Run an interactive demo of the suggestion system.\"\"\"\n",
    "    print(\"Loading Nepali words...\")\n",
    "    words = load_nepali_words()\n",
    "    print(f\"Loaded {len(words)} unique words\")\n",
    "    \n",
    "    # Load or train the model\n",
    "    model, char_to_idx, idx_to_char = load_trained_model()\n",
    "    if model is None:\n",
    "        print(\"Training new model...\")\n",
    "        model, char_to_idx, idx_to_char = train_model(words)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter a Nepali word (or 'quit' to exit): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        suggestions = get_suggestions(user_input, words, model, char_to_idx, idx_to_char)\n",
    "        print(f\"\\nSuggestions for '{user_input}':\")\n",
    "        for i, suggestion in enumerate(suggestions, 1):\n",
    "            print(f\"{i}. {suggestion}\")\n",
    "\n",
    "def test_examples():\n",
    "    \"\"\"Run some test examples.\"\"\"\n",
    "    print(\"Loading Nepali words...\")\n",
    "    words = load_nepali_words()\n",
    "    print(f\"Loaded {len(words)} unique words\")\n",
    "    \n",
    "    # Load or train the model\n",
    "    model, char_to_idx, idx_to_char = load_trained_model()\n",
    "    if model is None:\n",
    "        print(\"Training new model...\")\n",
    "        model, char_to_idx, idx_to_char = train_model(words)\n",
    "    \n",
    "    test_inputs = [\"ति\", \"नमस\", \"काठ\", \"नेपा\", \"भाष\"]\n",
    "    for input_text in test_inputs:\n",
    "        suggestions = get_suggestions(input_text, words, model, char_to_idx, idx_to_char)\n",
    "        print(f\"\\nInput: {input_text}\")\n",
    "        print(\"Suggestions:\", suggestions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Nepali Word Completion and Spelling Correction System\")\n",
    "    print(\"1. Run test examples\")\n",
    "    print(\"2. Start interactive demo\")\n",
    "    choice = input(\"Enter your choice (1 or 2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        test_examples()\n",
    "    elif choice == \"2\":\n",
    "        interactive_demo()\n",
    "    else:\n",
    "        print(\"Invalid choice!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
